{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "868a0c8e-a058-465c-8495-7e8d9a582ea3",
   "metadata": {},
   "source": [
    "# Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f26a99-e2a2-49db-88c3-6ec0c56ff735",
   "metadata": {},
   "source": [
    "Loan approval prediction is to predict whether the new loan application from new applicants should be rejected or approved. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07f6406b-212d-4d66-b5db-cc6fd45498a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/anaconda3/lib/python3.11/site-packages (25.2)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.11/site-packages (2.3.0)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /opt/anaconda3/lib/python3.11/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.11/site-packages (1.26.4)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/lib/python3.11/site-packages (3.10.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.23 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pillow>=8 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: seaborn in /opt/anaconda3/lib/python3.11/site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /opt/anaconda3/lib/python3.11/site-packages (from seaborn) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.2 in /opt/anaconda3/lib/python3.11/site-packages (from seaborn) (2.3.0)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /opt/anaconda3/lib/python3.11/site-packages (from seaborn) (3.10.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (23.1)\n",
      "Requirement already satisfied: pillow>=8 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas>=1.2->seaborn) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.11/site-packages (from pandas>=1.2->seaborn) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.11/site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-learn) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69f77b52-5905-4a45-91e3-4548863ea7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(font_scale=1)\n",
    "pd.set_option ('display.max_columns', None)\n",
    "pd.set_option ('display.max_rows', None)\n",
    "\n",
    "#import warnings to surpress warnings\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61e517c-0261-48d8-b3f4-e20c3f5813dc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 1. Data Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f2d8e81-02e8-42d8-9710-19eea5245e52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'loan_data_2007_2014.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Initializing dataset\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloan_data_2007_2014.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Shape of the Dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m rows and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m columns\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m df\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m5\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1662\u001b[0m     f,\n\u001b[1;32m   1663\u001b[0m     mode,\n\u001b[1;32m   1664\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1665\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1666\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1667\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1668\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1669\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1670\u001b[0m )\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    860\u001b[0m             handle,\n\u001b[1;32m    861\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    862\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    863\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    864\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    865\u001b[0m         )\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'loan_data_2007_2014.csv'"
     ]
    }
   ],
   "source": [
    "# Initializing dataset\n",
    "df = pd.read_csv(\"loan_data_2007_2014.csv\", index_col=0)\n",
    "print(f'\\n Shape of the Dataset: {df.shape[0]} rows and {df.shape[1]} columns')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902cfa03-7ca0-4ec0-8721-f221b7d42646",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Understand the data structure\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e39c36-2c25-4932-afce-80364b98adfb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Checking the shape of the dataframe\n",
    "df.shape\n",
    "print(f'row : {df.shape[0]}')\n",
    "print(f'column : {df.shape[1]}')\n",
    "\n",
    "# Exploring the descriptive statistics of the dataframe\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbd1f41-66f6-426b-8f0b-036fe577b4ca",
   "metadata": {},
   "source": [
    "# 2. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b05a87-cabb-458d-8f30-f422a548d4ec",
   "metadata": {},
   "source": [
    "- Numerical and categorical features\n",
    "- Data Cleaning\n",
    "- Imbalanced data and outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7df3ff-b0e6-4e60-809e-90db22ab15f6",
   "metadata": {},
   "source": [
    "## 2.1 Numerical & Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb326613-456c-48e7-9a39-9eb32698080e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate numerical & categorical feature columns\n",
    "numerical = df.select_dtypes (include = ['int64', 'float64'])\n",
    "categorical = df.select_dtypes (include = 'object')\n",
    "\n",
    "#Convert columns to list\n",
    "numerical_col = numerical.columns.to_list()\n",
    "categorical_col = categorical.columns.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e971eb0-deaa-45a7-8b36-ccd67ffa6fb6",
   "metadata": {},
   "source": [
    "### 2.1.1 Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06342333-05d4-4ce8-b5ff-d3c4e808bacf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Explore numerical features' descriptive\n",
    "print(f'rows: {numerical.shape[0]}')\n",
    "print (f'columns: {numerical.shape[1]}')\n",
    "\n",
    "numerical.describe().T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3d70c3-91d9-4453-87a0-4149e38c9851",
   "metadata": {},
   "source": [
    "### 2.1.2 Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fce775c-3207-44d7-8f4d-99603708d5a5",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Exploring the categorical features' descriptive\n",
    "\n",
    "print (f'rows: {categorical.shape[0]}')\n",
    "print (f'columns: {categorical.shape[1]}')\n",
    "\n",
    "categorical.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba56e97-ddb3-47b3-b5b2-c760722c9544",
   "metadata": {},
   "source": [
    "## 2.2 Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c158b7a2-5373-438d-9c87-036444983e0e",
   "metadata": {},
   "source": [
    "Checking and handling:\n",
    "- Duplicate values\n",
    "- Missing values\n",
    "- Irrelevant Features\n",
    "- Data leakage\n",
    "- Uninformative features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf35d77-c5b0-4d81-bbb9-9964bb2ad576",
   "metadata": {},
   "source": [
    "### 2.2.1 Duplicate Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0657681-d259-4d6e-8d85-2b41fd18f53b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e689528-a3c7-4c71-a371-aeb7373e0a25",
   "metadata": {},
   "source": [
    "### 2.2.2 Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ce8f2f-3890-4148-a12d-c898e79922f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#checking the total missing values in each feature\n",
    "sum_null = df.isnull().sum()\n",
    "\n",
    "# Calculate the percentage of the missing values in each feature\n",
    "missing_percent = (sum_null*100)/len(df)\n",
    "\n",
    "# Checking the data type for each feature\n",
    "data_type= [df[col].dtype for col in df.columns]\n",
    "\n",
    "# Create a new dataframe for the missing value\n",
    "data_isnull = pd.DataFrame({\"total_null\": sum_null, \"data_type\": data_type, \"missing_%\": missing_percent})\n",
    "\n",
    "#sort the percentage of missing values from the largest to the lowest\n",
    "data_isnull.sort_values(\"missing_%\", ascending = False, inplace = True)\n",
    "\n",
    "#display the missing value dataframe\n",
    "data_isnull_sort = data_isnull[data_isnull[\"missing_%\"]>0].reset_index()\n",
    "data_isnull_sort"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df08392-d3cd-48d0-b73a-99a2fae45071",
   "metadata": {},
   "source": [
    "#### Handling Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e130bb2-6be5-4ab9-a5fa-42ca9cd80281",
   "metadata": {},
   "source": [
    "##### a. Dropping 50% missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99bd7bb-2521-4087-a29a-f1ee63a76418",
   "metadata": {},
   "source": [
    "There are 21 columns that have more than 50% missing values. Thus, they are dropped from the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19ab88e-af6b-49f0-92f2-62fb69207c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop features with missing value ≥ 50%\n",
    "\n",
    "col_null = data_isnull.loc[data_isnull[\"missing_%\"]>=50].index.tolist()\n",
    "df.drop(columns = col_null, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b375b579-0533-4af6-9647-ca091984dd05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.shape\n",
    "print(f'row : {df.shape[0]}')\n",
    "print(f'column : {df.shape[1]}')\n",
    "print(f'numericals_features : {df.select_dtypes(include=[\"int64\", \"float64\"]).shape[1]}')\n",
    "print (f'categorical_features : {df.select_dtypes(include = \"object\").shape[1]}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2241fb01-f7a9-4d6b-909c-301a368b3e6d",
   "metadata": {},
   "source": [
    "##### b. Replacing Missing Values with 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6259029-5a43-4a56-899e-5fe2d52bd077",
   "metadata": {},
   "source": [
    "There are features with missing values that can be replaced with 0:\n",
    "\n",
    "- tot_coll_amt\t = Total collection amounts ever owed -> assumption: customers don't borrow.\n",
    "- tot_cur_bal\t     = Total current balance of all accounts  -> assumption: customers don't have any current balance.\n",
    "- total_rev_hi_lim = Total revolving high credit/credit limit -> assumption: customers don't have a revolving limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54112ffe-4409-46c4-8e61-41edd6375572",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Replacing the selected features with missing values with 0\n",
    "\n",
    "for item in [\"tot_coll_amt\", \"tot_cur_bal\", \"total_rev_hi_lim\"]:\n",
    "    df[item] = df[item].fillna(0)\n",
    "\n",
    "df[['tot_coll_amt', 'tot_cur_bal', 'total_rev_hi_lim']].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91a71d1-c0e6-4056-9e5b-e3f8a8855186",
   "metadata": {},
   "source": [
    "##### c. Handling missing values in numerical features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f707cc-fc66-4918-9da3-b66113395c64",
   "metadata": {},
   "source": [
    "Since the numerical features are in continuous values, the median of each of these features is used to replace the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a505d800-25f2-4344-b3b1-a61c133e5cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing the missing values in numerical features with median\n",
    "numericals = df.select_dtypes(include=['int64', 'float64'])\n",
    "for item in numericals:\n",
    "    df[item] = df[item].fillna(df[item].median())\n",
    "    \n",
    "print(f'numerical_features : {df.select_dtypes(include=[\"int64\", \"float64\"]).shape[1]}')\n",
    "numericals.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e3bff8-732d-4673-8e46-61d48cc28d6e",
   "metadata": {},
   "source": [
    "##### d. Handling missing values in categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6377793e-5be5-461d-8aee-bc3ffa25d0d8",
   "metadata": {},
   "source": [
    "The mode of each of categorical features is used to replace the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9799aca-16ee-44af-ac74-a2e3f7876f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing the missing values in categorical features with mode\n",
    "categoricals = df.select_dtypes(include=['object'])\n",
    "for item in categoricals:\n",
    "    df[item] = df[item].fillna(df[item].mode().iloc[0])\n",
    "    \n",
    "print(f'categorical_features : {df.select_dtypes(include=\"object\").shape[1]}')\n",
    "categoricals.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abee17d0-91d3-4972-8afc-bf1b9d1add56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c78444da-b058-4389-b2ef-1b597932dfd2",
   "metadata": {},
   "source": [
    "### 2.2.3 Irrelevant Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45349694-c7d4-4ec8-b69c-a5ff1e01f170",
   "metadata": {},
   "source": [
    "\n",
    "There are irrelevant features that don't have any correlation or association with the target features due to their nature of unique entries, no association with the target feature, leading to bias, etc. These features are:\n",
    "\n",
    "- id = A unique LC assigned ID for the loan listing. -> the unique entries with no association to the target feature\n",
    "- member_id = A unique LC assigned Id for the borrower member. -> too many unique entries\n",
    "- grade = LC assigned loan grade -> Instead, using sub_grade since it has more complete information\n",
    "- emp_title = The job title supplied by the Borrower when applying for the loan -> too many unique entries\n",
    "- url = URL for the LC page with listing data -> too many unique entries, no association with the target feature\n",
    "- zip_code = The first 3 numbers of the zip code provided by the borrower in the loan application. -> can lead to bias\n",
    "- addr_state = The state provided by the borrower in the loan application -> Can lead to bias\n",
    "- title = The loan title provided by the borrower -> too many unique entries\n",
    "- inq_last_6mths = The number of inquiries in the past 6 months.\n",
    "- issue_d = The month in which the loan was funded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313de2de-8f9d-4c11-a71f-850bd2411d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the irrelevant features\n",
    "df.drop(columns =[\"id\",\"member_id\",\"grade\",\"emp_title\",\"url\", \n",
    "                       \"zip_code\", \"addr_state\",\"title\", \n",
    "                       \"inq_last_6mths\",\"issue_d\"], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86f7337-889c-4891-8ed1-8fe9929bef25",
   "metadata": {},
   "source": [
    "### 2.2.4 Data Leakage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef351a4-6dbd-45fb-af30-9c3f6600669b",
   "metadata": {},
   "source": [
    "There are features are acquired after the loan is approved and are not available by the time when the model is making predictions. Thus, using these feature will lead to data leakage and unrealistic high accuracy during training and test, but poor performance during real-world prediction.\n",
    "\n",
    "- funded_amnt = The total amount committed to that loan at that point in time.\n",
    "- funded_amnt_inv = The total amount committed to that loan at that point in time by investor\n",
    "- out_prncp = Remaining outstanding principal for total amount funded\n",
    "- out_prncp_inv = Remaining outstanding principal for portion of total amount funded by investors\n",
    "- total_pymnt = Payments received to date for total amount funded\n",
    "- total_pymnt_inv = Payments received to date for portion of total amount funded by investors\n",
    "- total_rec_prncp = Principal received to date\n",
    "- total_rec_int = Interest received to date\n",
    "- total_rec_late_fee = Late fees received to date\n",
    "- recoveries = Indicates if a payment plan has been put in place for the loan\n",
    "- collection_recovery_fee = post charge off collection fee\n",
    "- tot_coll_amt = Total collection amounts ever owed\n",
    "- last_pymnt_amnt = Last total payment amount received\n",
    "- revol_util = Revolving line utilization rate, or the amount of credit the borrower is using relative to all available revolving credit.        \n",
    "- collections_12_mths_ex_med = Number of collections in 12 months excluding medical collections\n",
    "- last_pymnt_d = Last month's payment was received\n",
    "- next_pymnt_d = Next scheduled payment date\n",
    "- last_credit_pull_d = The most recent month LC pulled credit for this loan\n",
    "- initial_list_status = The initial listing status of the loan. Possible values are – Whole, Fractional\n",
    "- total_rev_hi_lim = total revolving high credit/credit limit\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c160e3-0feb-41b6-a10f-e5681661e8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the data leakage features\n",
    "df.drop(columns = ['funded_amnt', 'funded_amnt_inv', 'out_prncp', 'out_prncp_inv', \n",
    "                   'total_pymnt', 'total_pymnt_inv', 'total_rec_prncp', 'total_rec_int', \n",
    "                   'total_rec_late_fee', 'recoveries', 'collection_recovery_fee', \n",
    "                   'tot_coll_amt', 'last_pymnt_amnt', 'revol_util', 'collections_12_mths_ex_med',\n",
    "                   'last_pymnt_d','next_pymnt_d','last_credit_pull_d','initial_list_status',\n",
    "                   'total_rev_hi_lim' ], inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2da7ed-2817-418d-9b9b-caf245ecc9da",
   "metadata": {},
   "source": [
    "### 2.2.5 Uniformative Features for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73995595-a1c1-4edb-8acb-13f8ecee2b3f",
   "metadata": {},
   "source": [
    "There are categorical features that don't give enough information for classification predictions due to their lack of heterogeneity in values. \n",
    "\n",
    "- pymnt_plan = Indicates if a payment plan has been put in place for the loan -> 99.998% has the same value\n",
    "- policy_code = publicly available policy_code=1, new products not publicly available policy_code=2\" -> only 1 unique\n",
    "- application_type = Indicates whether the loan is an individual application or a joint application with two co-borrowers -> only 1 unique\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd406ab-adec-4825-8ece-622a6f24f2f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(8, 5))\n",
    "\n",
    "# Flatten the axes array to easily iterate\n",
    "axs = axs.flatten()\n",
    "\n",
    "# List of features to plot\n",
    "features = [\"pymnt_plan\", \"policy_code\", \"application_type\"]\n",
    "\n",
    "# Loop through features and axes\n",
    "for i, feature in enumerate(features):\n",
    "    df[feature].value_counts().plot.pie(\n",
    "        ax=axs[i], \n",
    "        autopct='%.3f%%',\n",
    "        startangle=90,\n",
    "        ylabel=\"\",  # hide default label\n",
    "        legend=False\n",
    "    )\n",
    "    axs[i].set_title(feature)\n",
    "\n",
    "# Overall title\n",
    "fig.suptitle('Distribution of Uniformative Features', fontsize=25)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8259c2-6c84-4827-b814-848d3b6b50de",
   "metadata": {},
   "source": [
    "Thus, these three features need to be dropped since they bring very little information as features to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcee309-f621-499d-aaac-31be1c63cf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the selected features\n",
    "\n",
    "df.drop(columns= [\"pymnt_plan\", \"policy_code\", \"application_type\"], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4327452e-9156-4f70-9f20-211278505115",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Checking the latest dataframe\n",
    "\n",
    "print(f'numericals_features : {df.select_dtypes(include=[\"int64\", \"float64\"]).shape[1]}')\n",
    "print (f'categorical_features : {df.select_dtypes(include = \"object\").shape[1]}')\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37941eb-f248-4cfb-b069-a2252295adfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking and removing duplicates in the dataset\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90706f46-83e3-4cf1-a40f-5a8c54ed0867",
   "metadata": {},
   "source": [
    "No duplicate values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c762c9-7b7b-4891-838f-e82ebf7cc088",
   "metadata": {},
   "source": [
    "## 2.3 Univariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7230aaf8-65c0-474d-966e-2e301bbd52c6",
   "metadata": {},
   "source": [
    "### 2.3.1 Numerical Features Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69ea2c1-505d-4887-a1a3-166aa6632ece",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import skew, kurtosis, probplot\n",
    "from IPython.display import display_html\n",
    "\n",
    "\n",
    "# Separate the latest numerical from categorical features\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Statistics for numerical features\n",
    "for col in num_cols:\n",
    "    data = df[col].dropna()\n",
    "\n",
    "    # Quantile Statistics\n",
    "    quantile_stats = {\n",
    "        'Minimum': data.min(),\n",
    "        '5-th Percentile': np.percentile(data, 5),\n",
    "        'Q1': data.quantile(0.25),\n",
    "        'Median': data.median(),\n",
    "        'Q3': data.quantile(0.75),\n",
    "        '95-th Percentile': np.percentile(data, 95),\n",
    "        'Maximum': data.max(),\n",
    "        'Range': data.max() - data.min(),\n",
    "        'IQR': data.quantile(0.75) - data.quantile(0.25)\n",
    "    }\n",
    "\n",
    "    # Descriptive Statistics\n",
    "    descriptive_stats = {\n",
    "        'Mean': data.mean(),\n",
    "        'Standard Deviation': data.std(),\n",
    "        'Variance': data.var(),\n",
    "        'Sum': data.sum(),\n",
    "        'Skewness': skew(data),\n",
    "        'Kurtosis': kurtosis(data),\n",
    "        'Coefficient of Variation': data.std() / data.mean()\n",
    "    }\n",
    "\n",
    "    # Convert to DataFrames\n",
    "    quantile_df = pd.DataFrame(quantile_stats.items(), columns=[\"Statistic\", \"Value\"])\n",
    "    descriptive_df = pd.DataFrame(descriptive_stats.items(), columns=[\"Statistic\", \"Value\"])\n",
    "\n",
    "    # Convert DataFrames to HTML tables\n",
    "    q_html = quantile_df.to_html(index=False)\n",
    "    d_html = descriptive_df.to_html(index=False)\n",
    "\n",
    "    # Display Side by Side\n",
    "    print(f\"\\n=== Statistics for {col} ===\")\n",
    "    display_html(f\"\"\"\n",
    "        <div style='display: flex; gap: 40px;'>\n",
    "            <div><h3>Quantile Statistics</h3>{q_html}</div>\n",
    "            <div><h3>Descriptive Statistics</h3>{d_html}</div>\n",
    "        </div>\n",
    "    \"\"\", raw=True)\n",
    "\n",
    "    # Plots\n",
    "    plt.figure(figsize=(16, 4))\n",
    "\n",
    "    # Histogram + KDE\n",
    "    plt.subplot(1, 3, 1)\n",
    "    sns.histplot(data, kde=True)\n",
    "    plt.title(f'Histogram & KDE: {col}')\n",
    "\n",
    "    # Boxplot\n",
    "    plt.subplot(1, 3, 2)\n",
    "    sns.boxplot(x=data)\n",
    "    plt.title(f'Boxplot: {col}')\n",
    "\n",
    "    # Q-Q Plot\n",
    "    plt.subplot(1, 3, 3)\n",
    "    probplot(data, dist=\"norm\", plot=plt)\n",
    "    plt.title(f'Q-Q Plot: {col}')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdc75d2-ed77-4669-8a15-67fb249dc278",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skew\n",
    "\n",
    "#Checking the skewness\n",
    "\n",
    "def check_skewness_outlier_effect(df):\n",
    "    skew_report = []\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "    for col in numeric_cols:\n",
    "        data = df[col].dropna()\n",
    "\n",
    "        # Skewness before\n",
    "        original_skew = skew(data)\n",
    "\n",
    "        # Outlier removal using IQR\n",
    "        Q1 = data.quantile(0.25)\n",
    "        Q3 = data.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        filtered_data = data[(data >= lower_bound) & (data <= upper_bound)]\n",
    "\n",
    "        # Skewness after\n",
    "        filtered_skew = skew(filtered_data)\n",
    "\n",
    "        # Interpretation\n",
    "        def interpret_skew(value):\n",
    "            if abs(value) < 0.5:\n",
    "                return \"Symmetric\"\n",
    "            elif abs(value) < 1:\n",
    "                return \"Moderately Skewed\"\n",
    "            else:\n",
    "                return \"Highly Skewed\"\n",
    "\n",
    "        skew_report.append({\n",
    "            'Feature': col,\n",
    "            'Skewness Before': round(original_skew, 4),\n",
    "            'Interpretation Before': interpret_skew(original_skew),\n",
    "            'Skewness After': round(filtered_skew, 4),\n",
    "            'Interpretation After': interpret_skew(filtered_skew),\n",
    "            'Change': round(original_skew - filtered_skew, 4)\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(skew_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43f8dd0-fa2c-443f-b712-9836b23316a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = check_skewness_outlier_effect(df)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8571fb-626e-408e-8d6f-1b495e9a28a0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "numerical_dataset = df.select_dtypes(include=['number'])\n",
    "correlation_ = numerical_dataset.corr()\n",
    "\n",
    "plt.figure(figsize=(25, 15))\n",
    "sns.heatmap(correlation_, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a577a65a-58c8-4f44-aa53-8be9c6e150a9",
   "metadata": {},
   "source": [
    "### 2.3.2 Categorical Features Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9571e068-6de0-4fa9-8409-203b2e3ac49e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the latest categorical features\n",
    "cat_cols = df.select_dtypes(exclude=[np.number]).columns\n",
    "\n",
    "# statistic function\n",
    "def _cat_stats_core(series: pd.Series) -> pd.DataFrame:\n",
    "    s_nn = series.dropna()\n",
    "    if s_nn.empty:\n",
    "        return pd.DataFrame({\n",
    "            \"Metric\": [\n",
    "                \"Unique (cardinality)\", \"Mode (top category)\", \"Mode count\", \n",
    "                \"Mode (%)\", \"Entropy (bits)\", \"Gini diversity\", \n",
    "                \"Avg. length\", \"Min length\", \"Max length\"\n",
    "            ],\n",
    "            \"Value\": [np.nan]*9\n",
    "        })\n",
    "    \n",
    "    vc = s_nn.value_counts()\n",
    "    mode_val = vc.index[0]\n",
    "    mode_count = int(vc.iloc[0])\n",
    "    mode_pct = mode_count / s_nn.shape[0] * 100\n",
    "    p = (vc / s_nn.shape[0]).values\n",
    "    entropy = -np.sum(p * np.log2(p))\n",
    "    gini = 1 - np.sum(p**2)\n",
    "    lens = s_nn.astype(str).str.len()\n",
    "    avg_len, min_len, max_len = lens.mean(), lens.min(), lens.max()\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"Metric\": [\n",
    "            \"Unique (cardinality)\", \"Mode (top category)\", \"Mode count\",\n",
    "            \"Mode (%)\", \"Entropy (bits)\", \"Gini diversity\",\n",
    "            \"Avg. length\", \"Min length\", \"Max length\"\n",
    "        ],\n",
    "        \"Value\": [\n",
    "            s_nn.nunique(), mode_val, mode_count, round(mode_pct, 2),\n",
    "            round(entropy, 4), round(gini, 4),\n",
    "            round(avg_len, 2), min_len, max_len\n",
    "        ]\n",
    "    })\n",
    "\n",
    "# Main function\n",
    "def analyze_categorical_features(df, top_n_bar=20, top_n_table=50, top_n_pie=10):\n",
    "    cat_cols = df.select_dtypes(include=[\"object\", \"category\"]).columns\n",
    "    if len(cat_cols) == 0:\n",
    "        print(\"No categorical features found.\")\n",
    "        return\n",
    "\n",
    "    for col in cat_cols:\n",
    "        s = df[col]\n",
    "        s_nn = s.dropna()\n",
    "\n",
    "        # Stats table\n",
    "        stats_df = _cat_stats_core(s)\n",
    "\n",
    "        # Value table\n",
    "        vc = s_nn.value_counts()\n",
    "        vt = (\n",
    "            vc.head(top_n_table)\n",
    "              .rename(\"Count\")\n",
    "              .to_frame()\n",
    "              .assign(Percentage=lambda d: (d[\"Count\"] / len(s_nn) * 100).round(2))\n",
    "              .reset_index()\n",
    "              .rename(columns={\"index\": col})\n",
    "        )\n",
    "\n",
    "        # Display tables side-by-side\n",
    "        display_html(\n",
    "            f\"\"\"\n",
    "            <div style=\"display:flex; gap:40px; align-items:flex-start;\">\n",
    "                <div style=\"flex:0 0 360px;\">\n",
    "                    <h3>Stats for {col}</h3>\n",
    "                    {stats_df.to_html(index=False)}\n",
    "                </div>\n",
    "                <div style=\"flex:1;\">\n",
    "                    <h3>Value Table (top {min(top_n_table, len(vc))})</h3>\n",
    "                    {vt.to_html(index=False)}\n",
    "                </div>\n",
    "            </div>\n",
    "            \"\"\",\n",
    "            raw=True\n",
    "        )\n",
    "\n",
    "        # Bar plot\n",
    "        plt.figure(figsize=(14, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        vc_bar = vc.head(top_n_bar)[::-1]\n",
    "        plt.barh(vc_bar.index.astype(str), vc_bar.values)\n",
    "        plt.title(f'Bar Plot (Top {min(top_n_bar, len(vc))}): {col}')\n",
    "        plt.xlabel('Count')\n",
    "        plt.ylabel('Category')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "analyze_categorical_features(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9abe4a-6e52-48a7-842c-4c5355a08698",
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "import pandas as pd\n",
    "\n",
    "def check_categorical_imbalance(df, dominance_threshold=3, entropy_threshold=1.0):\n",
    "    cat_report = []\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "    for col in categorical_cols:\n",
    "        counts = df[col].value_counts()\n",
    "        dominance_ratio = counts.max() / counts.min() if counts.min() > 0 else None\n",
    "        ent = entropy(counts)\n",
    "\n",
    "        # Determine imbalance flag\n",
    "        if (dominance_ratio and dominance_ratio >= dominance_threshold) or (ent <= entropy_threshold):\n",
    "            flag = \"Imbalanced\"\n",
    "        else:\n",
    "            flag = \"Balanced\"\n",
    "\n",
    "        cat_report.append({\n",
    "            'Feature': col,\n",
    "            'Unique Categories': len(counts),\n",
    "            'Entropy': round(ent, 3),\n",
    "            'Dominance Ratio': round(dominance_ratio, 2) if dominance_ratio else None,\n",
    "            'Most Common Category': counts.idxmax(),\n",
    "            'Most Common Freq (%)': round((counts.max()/counts.sum())*100, 2),\n",
    "            'Imbalance Flag': flag\n",
    "        })\n",
    "    return pd.DataFrame(cat_report)\n",
    "\n",
    "report = check_categorical_imbalance(df)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1408a08-1638-4f14-931f-9901aa194dfc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 2.4 Bivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501a0993-320e-491c-884f-adcbdc74d8a4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from IPython.display import display_html\n",
    "\n",
    "def bivariate_analysis(df, target=\"loan_status\"):\n",
    "    df = df.copy()\n",
    "    df = df[df[target].isin([\"Fully Paid\", \"Charged Off\"])]\n",
    "    df[target] = df[target].map({\n",
    "        \"Fully Paid\": \"Good Loan\",\n",
    "        \"Charged Off\": \"Bad Loan\"\n",
    "    })\n",
    "    \n",
    "    # Separate features\n",
    "    num_cols = df.select_dtypes(include=[\"float\", \"int\"]).columns.drop(target, errors=\"ignore\")\n",
    "    cat_cols = df.select_dtypes(include=[\"object\", \"category\"]).columns.drop(target, errors=\"ignore\")\n",
    "\n",
    "    # --- Numerical Features ---\n",
    "    for col in num_cols:\n",
    "        print(f\"\\n=== Numerical Feature: {col} vs {target} ===\")\n",
    "\n",
    "        # Stats per class\n",
    "        stats_good = df[df[target] == \"Good Loan\"][col].describe().round(2)\n",
    "        stats_bad = df[df[target] == \"Bad Loan\"][col].describe().round(2)\n",
    "        stats_df = pd.DataFrame({\"Good Loan\": stats_good, \"Bad Loan\": stats_bad})\n",
    "        \n",
    "        # Display side-by-side table\n",
    "        display_html(f\"\"\"\n",
    "        <h4>Descriptive Stats for {col}</h4>\n",
    "        {stats_df.to_html()}\n",
    "        \"\"\", raw=True)\n",
    "\n",
    "        # Boxplot\n",
    "        plt.figure(figsize=(14, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        sns.boxplot(x=target, y=col, data=df, palette=\"Set2\")\n",
    "        plt.title(f\"Boxplot of {col} by Loan Status\")\n",
    "\n",
    "        # KDE Plot\n",
    "        plt.subplot(1, 2, 2)\n",
    "        for label, color in zip([\"Good Loan\", \"Bad Loan\"], [\"green\", \"red\"]):\n",
    "            sns.kdeplot(df[df[target]==label][col], label=label, fill=True, alpha=0.4, color=color)\n",
    "        plt.title(f\"KDE of {col} by Loan Status\")\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # --- Categorical Features ---\n",
    "    for col in cat_cols:\n",
    "        print(f\"\\n=== Categorical Feature: {col} vs {target} ===\")\n",
    "\n",
    "        # Crosstab for counts & percentage\n",
    "        ct = pd.crosstab(df[col], df[target])\n",
    "        pct = ct.div(ct.sum(axis=1), axis=0).mul(100).round(2)\n",
    "        combined = ct.astype(str) + \" (\" + pct.astype(str) + \"%)\"\n",
    "        \n",
    "        display_html(f\"\"\"\n",
    "        <h4>Counts & Percentages for {col}</h4>\n",
    "        {combined.to_html()}\n",
    "        \"\"\", raw=True)\n",
    "\n",
    "        # Bar Plot\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        ct.plot(kind=\"bar\", stacked=False, figsize=(10, 5), color=[\"green\", \"red\"])\n",
    "        plt.title(f\"{col} Distribution by Loan Status\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.xlabel(col)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e14093-db49-411f-86a1-13969fb16d02",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "bivariate_analysis(df, target=\"loan_status\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838213dc-9f5d-4638-959d-a500e53255f8",
   "metadata": {},
   "source": [
    "# 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4770dace-c0f0-489e-82c4-6f9ed94e44f8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "- Feature transformation\n",
    "- Feature Encoding\n",
    "- Feature selection\n",
    "- Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771d0e91-eddb-4128-84e7-531e3c74474b",
   "metadata": {},
   "source": [
    "## 3.1 Feature Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053a9ef1-85b2-4e35-a42e-91d0c6bca0de",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Transforming categorical features to be numerical features.\n",
    "- term\n",
    "- earliest_cr_line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515950cf-f961-49c9-9da6-023a2ae6eb47",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 3.1.1 Feature: term"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11adeee2-ff68-40e1-8002-7131dcd13061",
   "metadata": {},
   "source": [
    "Converting the term value from 36 months or 60 months to be 36 or 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3075dc4d-be3f-4158-a4be-3ea84349c1d4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Convert feature term\n",
    "df['term'] = df['term'].apply(lambda x: int(x.split()[0]))\n",
    "df['term'] = df['term'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b2a0fa-c955-4883-bc0e-c3249d9f431f",
   "metadata": {},
   "source": [
    "### 3.1.2 Feature: earliest_cr_line  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfc2f33-1bc1-4a39-bbcd-2aa64abebe23",
   "metadata": {},
   "source": [
    "Extracting the feature earliest_cr_line date for the year only with 4-digit year format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e2c4b5-0b91-4cb2-8cea-aac906300987",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert strings like \"Jan-85\" to proper datetime first\n",
    "df['earliest_cr_line'] = pd.to_datetime(df['earliest_cr_line'], format='%b-%y')\n",
    "\n",
    "# Extract the 4-digit year\n",
    "df['earliest_cr_line'] = df['earliest_cr_line'].dt.year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f7fb85-a1da-41e2-9201-2247ffb65488",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3.2 Feature Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb551a1-73e5-4b18-8644-ad80db85f862",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Feature encoding:\n",
    "- Label encoding\n",
    "- One-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e6ce2f-5c47-4b09-89fa-dfe2f7fc76de",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 3.2.1 Label Encoding:\n",
    "- Feature: sub_grade\n",
    "- Feature: emp_lenght\n",
    "- Feature: verification_status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5c7cb7-9ed2-4ffc-b4a8-645f4effe199",
   "metadata": {},
   "source": [
    "#### a. Feature: sub_grade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfad58d4-60d4-4aee-9ae4-1ca3e0accdd3",
   "metadata": {},
   "source": [
    "Label Encoding scale: A1 = 35 (the best) → G5 = 1 (the worst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29406ce0-5fa3-456a-884d-c0bae63218e2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data Sub-Grade\n",
    "sub_grades = [\n",
    "    \"A1\", \"A2\", \"A3\", \"A4\", \"A5\",\n",
    "    \"B1\", \"B2\", \"B3\", \"B4\", \"B5\",\n",
    "    \"C1\", \"C2\", \"C3\", \"C4\", \"C5\",\n",
    "    \"D1\", \"D2\", \"D3\", \"D4\", \"D5\",\n",
    "    \"E1\", \"E2\", \"E3\", \"E4\", \"E5\",\n",
    "    \"F1\", \"F2\", \"F3\", \"F4\", \"F5\",\n",
    "    \"G1\", \"G2\", \"G3\", \"G4\", \"G5\"]\n",
    "\n",
    "# Label Encoding\n",
    "sub_grade_labels = {grade: i for i, grade in enumerate(reversed(sub_grades), start=1)}\n",
    "df['sub_grade'] = df['sub_grade'].map(sub_grade_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbf211c-e025-47dc-b6a0-684e40e956fc",
   "metadata": {},
   "source": [
    "#### b. Feature: emp_lenght"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201a0a58-cd14-446d-9350-8b57d8ce6266",
   "metadata": {},
   "source": [
    "Convert emp_lenght to be interger. \n",
    "- 0 and <1 year are considered 0\n",
    "- 10+ years are considered 10\n",
    "- the rest are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6662bd70-5229-46bc-8bd0-75196284cbe6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "emp_length = {\n",
    "    '0' : 0,\n",
    "    '< 1 year': 0,\n",
    "    '1 year': 1,\n",
    "    '2 years': 2,\n",
    "    '3 years': 3,\n",
    "    '4 years': 4,\n",
    "    '5 years': 5,\n",
    "    '6 years': 6,\n",
    "    '7 years': 7,\n",
    "    '8 years': 8,\n",
    "    '9 years': 9,\n",
    "    '10+ years': 10\n",
    "}\n",
    "\n",
    "# Mapping the new 'emp_lenght' into the dataset\n",
    "df['emp_length'] = df['emp_length'].map(emp_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c3208d-8d78-43a7-9bd4-5ec4503170dd",
   "metadata": {},
   "source": [
    "#### c. Feature: verification_status\n",
    "\n",
    "Convert verification_status to interger with label encoding:\n",
    "- Verified = 0\n",
    "- Not Verified = 1\n",
    "- Source Verified = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58326d7d-46bb-4247-b1ae-dbff66ee2292",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "verification_status = {\n",
    "    'Verified': 0,\n",
    "    'Not Verified': 1,\n",
    "    'Source Verified': 2\n",
    "}\n",
    "\n",
    "# Mapping the new 'verification_status' into the dataset\n",
    "df['verification_status'] = df['verification_status'].map(verification_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3141cc-3cab-403f-ad2a-4939371a2134",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b950365f-7942-418e-ba32-c957c9587128",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 3.2.2 One-Hot Encoding:\n",
    "- Feature: home_ownership\n",
    "- Feature: purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17b3663-ad29-4e0e-adfe-1a4a1482a118",
   "metadata": {},
   "source": [
    "#### Feature: home_ownership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a3424a-2fee-46b9-982e-9a7aaaf80291",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Changing home_ownership to 4 categories\n",
    "home_ownership = {\n",
    "    'OTHER': 'OTHER',\n",
    "    'NONE': 'OTHER',\n",
    "    'ANY': 'OTHER'\n",
    "}\n",
    "\n",
    "# Mapping the new 'home_ownership' into the dataset\n",
    "df['home_ownership'] = df['home_ownership'].replace(home_ownership)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3203b378-d632-40e1-b61a-4e5afbad1908",
   "metadata": {},
   "source": [
    "#### One-Hot Encoding Feature: home_ownership & Feature: purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34251946-3226-4eb7-9624-15fcd256fced",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, columns=['home_ownership',\n",
    "                                    'purpose'], drop_first=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3db0963-a025-498d-8ada-d549c33fc5a8",
   "metadata": {},
   "source": [
    "## Final check on latest dataframe (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ca12cc-d97f-4370-823e-199a1da5117f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab32ee03-e54c-4640-90c0-7356d13d0c1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e7d7e2-2816-483c-840a-fe37f3cef254",
   "metadata": {},
   "source": [
    "Categorical features have been transformed into numerical. Thus, all features are in numerical type now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3deb585-9ddb-4fb2-869d-e93a199d644c",
   "metadata": {},
   "source": [
    "## 3.3 Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd36ed4c-806d-4fe7-97d2-7d72be04f4b0",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "numerical_dataset = df.select_dtypes(include=['number'])\n",
    "correlation_ = numerical_dataset.corr()\n",
    "\n",
    "plt.figure(figsize=(25, 15))\n",
    "sns.heatmap(correlation_, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b7a511-9cbd-49ae-ba8b-ba9d1f3a0ddb",
   "metadata": {},
   "source": [
    "The correlation hatmap shows that feature 'loan_amnt' and feature 'installment' have strong correlation (0.95). Thus, only one feature is needed, and feature 'installment' will be dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b043fa21-6e3b-46bc-9d25-780177996753",
   "metadata": {},
   "source": [
    "## 3.4 TARGET FEATURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81c47fb-d40c-48d0-84f8-b07769bba3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Dataframe with values in loan_status only 'Fully Paid' and 'Charged Off'\n",
    "\n",
    "loan_data= df[df['loan_status'].isin(['Fully Paid', 'Charged Off'])].copy()\n",
    "\n",
    "#Mapping the target feature\n",
    "loan_data['target'] = loan_data['loan_status'].map({\n",
    "    'Fully Paid': 0,\n",
    "    'Charged Off': 1,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7590596e-0275-430f-930a-95f981c84e35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loan_data.drop(columns= ['loan_status'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0930e9d7-4385-4828-83dc-03c27331bef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_data['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac24b11f-ad74-468b-98ec-c31f09bcb46e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Separate Features & Target\n",
    "X = loan_data.drop(columns=['target'])\n",
    "y = loan_data['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5e6d47-5124-4da8-8fdb-7f4d5d6e6e08",
   "metadata": {},
   "source": [
    "## 3.5 Best Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9eabf3a-c2da-4567-9d00-6df4dcc704a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746c0e42-d3bd-4c57-82a1-985b78a75bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = SelectKBest(score_func=mutual_info_classif, k=20)  \n",
    "X_new = selector.fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7dd960-9698-4beb-b21a-dd4eb4157d51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores = selector.scores_\n",
    "feature_scores = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Score': scores\n",
    "})\n",
    "\n",
    "# Sort by highest score\n",
    "feature_scores = feature_scores.sort_values(by='Score', ascending=False)\n",
    "print(feature_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa8b178-fa26-477c-93c2-0fffb709c330",
   "metadata": {},
   "source": [
    "### Split Train and Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77c2dd2-2743-4680-b26b-cf336c82c53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c01972-8ef2-4d95-bdbf-dcf4bde2f5e1",
   "metadata": {},
   "source": [
    "# 4. Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bd9fa3-24ca-492f-99b5-a50816711d9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c5e26b-8156-4f6a-993c-ef96cbac343e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install xgboost\n",
    "!{sys.executable} -m pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757f1653-6bf5-433d-90bc-ac7b03408555",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, roc_auc_score, roc_curve, auc, make_scorer,confusion_matrix,precision_recall_curve\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "#XGB\n",
    "import xgboost as xgb\n",
    "\n",
    "import joblib\n",
    "\n",
    "#hyperparameter\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, StratifiedKFold, KFold, cross_val_score, cross_validate\n",
    "\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "#import itertools\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c554f2-df52-41ee-8774-b2c79c958f25",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Definining Functions and Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad314b4-1642-4a93-9e73-2f16eb375560",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Evaluation Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e601e15c-c57c-404c-ad80-d35c8be1d450",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluates the performance of a trained model on test data using various metrics.\n",
    "    \"\"\"\n",
    "    # Predictions\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "\n",
    "    # Probabilities (only used for ROC-AUC)\n",
    "    y_prob_train = model.predict_proba(X_train)[:, 1]\n",
    "    y_prob_test = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Get classification report\n",
    "    report_train = classification_report(y_train, y_pred_train, output_dict=True)\n",
    "    report_test = classification_report(y_test, y_pred_test, output_dict=True)\n",
    "    \n",
    " \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Extracting metrics\n",
    "    train_metrics = {\n",
    "        \"precision_0\": report_train[\"0\"][\"precision\"],\n",
    "        \"precision_1\": report_train[\"1\"][\"precision\"],\n",
    "        \"recall_0\":   report_train[\"0\"][\"recall\"],\n",
    "        \"recall_1\":   report_train[\"1\"][\"recall\"],\n",
    "        \"f1_0\":   report_train[\"0\"][\"f1-score\"],\n",
    "        \"f1_1\":   report_train[\"1\"][\"f1-score\"],\n",
    "        \"macro_avg_precision\":  report_train[\"macro avg\"][\"precision\"],\n",
    "        \"macro_avg_recall\":   report_train[\"macro avg\"][\"recall\"],\n",
    "        \"macro_avg_f1\":   report_train[\"macro avg\"][\"f1-score\"],\n",
    "        \"accuracy\": accuracy_score(y_train, y_pred_train),\n",
    "        \"ROC-AUC\": roc_auc_score(y_train, y_prob_train),\n",
    "        \"precision_recall\": precision_recall_curve(y_train, y_prob_train)\n",
    "        \n",
    "    }\n",
    "\n",
    "\n",
    "    test_metrics = {\n",
    "        \"precision_0\": report_test[\"0\"][\"precision\"],\n",
    "        \"precision_1\": report_test[\"1\"][\"precision\"],\n",
    "        \"recall_0\":  report_test[\"0\"][\"recall\"],\n",
    "        \"recall_1\":  report_test[\"1\"][\"recall\"],\n",
    "        \"f1_0\":  report_test[\"0\"][\"f1-score\"],\n",
    "        \"f1_1\":  report_test[\"1\"][\"f1-score\"],\n",
    "        \"macro_avg_precision\":  report_test[\"macro avg\"][\"precision\"],\n",
    "        \"macro_avg_recall\":  report_test[\"macro avg\"][\"recall\"],\n",
    "        \"macro_avg_f1\":  report_test[\"macro avg\"][\"f1-score\"],\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred_test),\n",
    "        \"ROC-AUC\": roc_auc_score(y_test, y_prob_test),\n",
    "        \"precision_recall\": precision_recall_curve(y_test, y_prob_test)\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "    # Convert dictionary to dataframe\n",
    "    metrics = pd.DataFrame([train_metrics,test_metrics],  index=[\"Train\", \"Test\"])\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d23148-34cb-4c80-adda-d74ff6ab434c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e32ddd-ca71-42ad-93b1-91e7f9d05e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix function\n",
    "def plot_confusion_matrix(c_matrix,\n",
    "                          target_names,\n",
    "                          title = 'Confusion Matrix',\n",
    "                          cmap= None, \n",
    "                          normalize= True):\n",
    "    \n",
    "    #np.trace = sum of all the elements of a diagonal of given matrix\n",
    "    accuracy = np.trace(c_matrix) / np.sum(c_matrix).astype('float')\n",
    "    misclass = 1 - accuracy\n",
    "    \n",
    "    #get colormap instance\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('coolwarm')\n",
    "        \n",
    "    #image size\n",
    "    plt.figure(figsize=(8,6))\n",
    "    \n",
    "    #display data as an image\n",
    "    #data is resampled to the pixel size of the image on the figure canvas \n",
    "    #'nearest' interpolation is used if the number of display pixels is at least three times the size of the data array\n",
    "    plt.imshow(c_matrix, interpolation='nearest', cmap= cmap)\n",
    "    \n",
    "    plt.title(title)\n",
    "    \n",
    "    #add a colorbar to a plot\n",
    "    plt.colorbar()\n",
    "    \n",
    "    #set tick locations\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "    \n",
    "    #percentage with normalize\n",
    "    #np.newaxis increase the dimensions of an array by adding new axes.\n",
    "    if normalize:\n",
    "        c_matrix = c_matrix.astype('float') / c_matrix.sum(axis =1)[:, np.newaxis]\n",
    "    \n",
    "    #threshold\n",
    "    threshold = c_matrix.max()/1 if normalize else c_matrix.max()/2\n",
    "    \n",
    "    #itertools.product returns the cartesian product of the input iterables\n",
    "    #The Cartesian product is the set of all combinations of elements from multiple sets\n",
    "    for i,j in itertools.product(range(c_matrix.shape[0]), range(c_matrix.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(c_matrix[i,j]),\n",
    "                     horizontalalignment = 'center',\n",
    "                     color = 'white' if c_matrix[i,j] > threshold else 'black')\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(c_matrix[i,j]),\n",
    "                     horizontalalignment = 'center',\n",
    "                     color = 'white' if c_matrix[i,j] > threshold else 'black')\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5169fba-d85f-4162-92c7-5d372383493b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Hyperparameter Tuning Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a6b1af-3247-4470-a022-c396f27a9f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the function of hyperparameter tuning with RandomizedSearch\n",
    "\n",
    "def tune_clf_hyperparameters_random(clf, param_distributions, X_train, y_train,\n",
    "                                    scoring='recall', n_splits=3, n_iter=10, random_state=42):\n",
    "    \n",
    "    #cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    cv = 3\n",
    "\n",
    "    clf_random = RandomizedSearchCV(\n",
    "        estimator=clf,\n",
    "        param_distributions=param_distributions,\n",
    "        n_iter=n_iter,\n",
    "        cv=cv,\n",
    "        scoring=scoring,\n",
    "        n_jobs=-1, \n",
    "        random_state=random_state,\n",
    "        refit = 'recall'\n",
    "    )\n",
    "\n",
    "    clf_random.fit(X_train, y_train)\n",
    "    best_hyperparameters = clf_random.best_params_\n",
    "    return clf_random.best_estimator_, best_hyperparameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be11dea5-59e5-41c3-b34a-a210b3b252c7",
   "metadata": {},
   "source": [
    "## 4.1 Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f34916-3aa7-41fc-9473-2cab982f2f17",
   "metadata": {},
   "source": [
    "### 4.1.1 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4728c0fd-77f2-4c05-8002-5b101c4062d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base DT model\n",
    "dt_base = DecisionTreeClassifier(class_weight='balanced',random_state=0)\n",
    "dt_base.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4cc093-2bae-48b1-9596-a68f5abe9d7b",
   "metadata": {},
   "source": [
    "### 4.1.2 Hyperparameter Tuning with RandomizedSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b583bb3a-29a0-42b0-84f0-ce586f2b246f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the parameter options\n",
    "param_dist_dt = {\n",
    "    'max_depth': randint(0,5),\n",
    "    'splitter': ['best'],\n",
    "    'min_samples_split': randint(2, 5),\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'min_samples_leaf': randint (1, 5)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bec4774-1563-40bb-8701-2036f6861ec2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Call the function for hyperparameter tuning\n",
    "best_dt, best_dt_hyperparams = tune_clf_hyperparameters_random(dt_base, param_dist_dt, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13ab7d0-1adf-41d2-b8ea-d2575e2bb3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Decision Tree Optimal Hyperparameters: \\n', best_dt_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7d5470-1e63-4950-b9d3-e584f09dabb8",
   "metadata": {},
   "source": [
    "### 4.1.3 Decision Tree Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c234966-e3d3-44a7-894d-afabe86261cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train= best_dt.predict(X_train)\n",
    "y_pred_test = best_dt.predict(X_test)\n",
    "\n",
    "print('Train Classification Report: \\n',classification_report(y_train, y_pred_train))\n",
    "print('Test Classification Report: \\n',classification_report(y_test,y_pred_test ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60256b2c-0cd8-48ba-a08a-e227bbab5df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_evaluation = evaluate_model(best_dt, X_train, y_train, X_test, y_test)\n",
    "dt_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985c0108-314d-4cb5-a305-94bd39c58486",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Confusion matrix for test prediction\n",
    "\n",
    "cm_dt = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "plot_confusion_matrix(\n",
    "    cm_dt,\n",
    "    target_names=['Good Loan', 'Bad Loan'], \n",
    "    title='Confusion Matrix',\n",
    "    normalize=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3290c609-30f6-45c1-aa07-89e75ff033ca",
   "metadata": {},
   "source": [
    "## 4.2 Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667a7e5c-afa7-45c6-9a04-cd9966cdb8cf",
   "metadata": {},
   "source": [
    "### 4.2.1 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6c2723-b0bd-4f49-9045-b3bbb5af0dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_base = RandomForestClassifier(class_weight = {0: 1, 1: 5}, random_state=0)\n",
    "rf_base = rf_base.fit(X_train, y_train)\n",
    "rf_base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8cd772-00db-46e4-81b2-52200048b68a",
   "metadata": {},
   "source": [
    "### 4.2.2. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6685068f-40d4-448e-b239-bf08fab8c38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist_rf = {\n",
    "    'n_estimators': [100],\n",
    "    'criterion': ['gini', 'entropy', 'logloss'],\n",
    "   'max_depth': randint(0,5),\n",
    "    'min_samples_split':randint(2,5),\n",
    "    'min_samples_leaf':randint(1,5),\n",
    "    'bootstrap': [True, False],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2619775b-0352-493c-b864-240db89db356",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using the tune_clf_hyperparameters function to get the best estimator\n",
    "best_rf, best_rf_hyperparams = tune_clf_hyperparameters_random(rf_base, param_dist_rf, X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c51d4a-c0b8-48f6-a944-a442ef610466",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RF Optimal Hyperparameters: \\n', best_rf_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb087af-9c6f-426c-8b22-ca3f8cb0f669",
   "metadata": {},
   "source": [
    "### 4.2.3 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c68e19-3acb-4535-b0de-d4a0ef6c3d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train= best_rf.predict(X_train)\n",
    "y_pred_test = best_rf.predict(X_test)\n",
    "\n",
    "print('Train Classification Report: \\n',classification_report(y_train, y_pred_train))\n",
    "print('Test Classification Report: \\n',classification_report(y_test,y_pred_test ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc9ddfe-0dc7-4f97-8a71-1a09c1533cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_evaluation = evaluate_model(best_rf, X_train, y_train, X_test, y_test)\n",
    "rf_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb8d4ad-62eb-4352-8e69-9808568a63c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7df7dca-da27-4457-a588-60e90ac4d506",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Confusion matrix for test prediction\n",
    "\n",
    "cm_rf = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "plot_confusion_matrix(\n",
    "    cm_rf,\n",
    "    target_names=['Good Loan', 'Bad Loan'], \n",
    "    title='Confusion Matrix',\n",
    "    normalize=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa00160-a913-4391-8f88-d5bec22070c5",
   "metadata": {},
   "source": [
    "## 4.3 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de03e46e-e592-4778-8422-18494d5ac79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the function of hyperparameter tuning with RandomizedSearch\n",
    "\n",
    "def tune_clf_hyperparameters_random(clf, param_distributions, X_train, y_train,\n",
    "                                    scoring='recall', n_splits=3, n_iter=20, random_state=42):\n",
    "    \n",
    "    cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "   \n",
    "\n",
    "    clf_random = RandomizedSearchCV(\n",
    "        estimator=clf,\n",
    "        param_distributions=param_distributions,\n",
    "        n_iter=n_iter,\n",
    "        cv=cv,\n",
    "        scoring=scoring,\n",
    "        n_jobs=-1, \n",
    "        random_state=random_state,\n",
    "        refit = 'roc',\n",
    "        \n",
    "    )\n",
    "\n",
    "    clf_random.fit(X_train, y_train)\n",
    "    best_hyperparameters = clf_random.best_params_\n",
    "    return clf_random.best_estimator_, best_hyperparameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da088291-1957-4bb8-8240-56011824c4ee",
   "metadata": {},
   "source": [
    "### 4.3.1 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e24b214-242f-4621-b24e-cd3f216510ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "xgb_base = xgb.XGBClassifier(scale_pos_weight=4.3,\n",
    "    eval_metric='aucpr',\n",
    "    use_label_encoder=False,\n",
    "    random_state=42)\n",
    "#xgb_base.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c069314-363c-4ae0-8362-747931b055dc",
   "metadata": {},
   "source": [
    "### 4.3.2 Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e128f9c3-c2c2-43b9-a6e4-417bc751fa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_param_dist = {\n",
    "    'max_depth': randint(5,10),           # Range [3, 10)\n",
    "    'learning_rate': uniform(0.01, 1),   # Range [0.01, 0.31)\n",
    "    'n_estimators': randint(50, 200),\n",
    "    'subsample': uniform(0.4, 1.5),\n",
    "    'objective':['binary:logistic'], # Range [0.6, 1.0)\n",
    "     'colsample_bytree': [0.3,0.6, 1.0]\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e62b43-c003-4e34-97bd-96bc8e3fbb57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for XGB\n",
    "best_xgb, best_xgb_hyperparams = tune_clf_hyperparameters_random(xgb_base, xgb_param_dist, X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288d585b-ce82-41a7-afe3-00c4c7184f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('XGB Optimal Hyperparameters: \\n', best_xgb_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e7b13b-431a-40af-b0e8-4ad9b008f764",
   "metadata": {},
   "source": [
    "### 4.3.3 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2348766-e1cf-4bf4-b9ce-1da118e965fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train= best_xgb.predict(X_train)\n",
    "y_pred_test = best_xgb.predict(X_test)\n",
    "\n",
    "print('Train Classification Report: \\n',classification_report(y_train, y_pred_train))\n",
    "print('Test Classification Report: \\n',classification_report(y_test,y_pred_test ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d2144d-e5ee-488a-99c8-3275249d5b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_evaluation = evaluate_model(best_xgb, X_train, y_train, X_test, y_test)\n",
    "xgb_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1e5ef6-ad34-4d39-9cf6-18d83f059187",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion matrix for test prediction\n",
    "\n",
    "cm_xgb = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "plot_confusion_matrix(\n",
    "    cm_xgb,\n",
    "    target_names=['Good Loan', 'Bad Loan'], \n",
    "    title='Confusion Matrix',\n",
    "    normalize=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bf4c4a-6cb6-4ba4-a4c1-f14135c0dd58",
   "metadata": {},
   "source": [
    "## 4.4. Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bcdd57-800a-429f-93fa-e8b3a67c8bdc",
   "metadata": {},
   "source": [
    "### 4.4.1 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b347bc-57b7-4cb8-8d6b-efaf59219bde",
   "metadata": {},
   "source": [
    "#### Random Forest as Estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114ba72c-ed48-46c0-83ca-28642f10daff",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators =[('rf', RandomForestClassifier(\n",
    "    class_weight= 'balanced', \n",
    "    criterion = 'gini',\n",
    "    max_depth = 1,\n",
    "    min_samples_split = 3,\n",
    "    min_samples_leaf = 4,\n",
    "    bootstrap = False,\n",
    "))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2303230-b366-42f4-959f-60521a9c3926",
   "metadata": {},
   "source": [
    "#### XGB as Final Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae97ed7-9cbf-4d16-b10b-76007402168c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_estimator = xgb.XGBClassifier(scale_pos_weight=4.3,\n",
    "    eval_metric=\"auc\",\n",
    "    use_label_encoder=False,\n",
    "    random_state=42,\n",
    "    max_depth = 7,         \n",
    "    learning_rate =  0.0564,  \n",
    "    n_estimators= 184,\n",
    "    subsample = 0.655786,\n",
    "    objective= 'binary:logistic',\n",
    "    colsample_bytree =  0.3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7baf72-5b18-4d7b-8cc8-d7cdb4604bba",
   "metadata": {},
   "source": [
    "#### Model Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4da3e6-3499-4654-bd0b-5ad03ae4a0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import  StackingClassifier\n",
    "stack_model = StackingClassifier(\n",
    "    estimators=estimators,\n",
    "    final_estimator= final_estimator,\n",
    "    passthrough=True,          \n",
    "    n_jobs=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84958c2-3527-41a4-8e65-1e3e40c79568",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    #('scaler', RobustScaler()),    \n",
    "    #('smote', SMOTE(random_state=42)),\n",
    "    ('model', stack_model)\n",
    "])\n",
    "\n",
    "pipeline =pipeline.fit(X_train, y_train)\n",
    "pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d311e530-d715-4a47-9649-c2333b0c8e51",
   "metadata": {},
   "source": [
    "### 4.4.2 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7249a22f-e67e-4f50-9a59-eaf8fca6bb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = pipeline.predict(X_test)\n",
    "y_pred_train = pipeline.predict(X_train)\n",
    "\n",
    "print(\"Train Classification Report:\\n\", classification_report(y_train, y_pred_train))\n",
    "print(\"Test Classification Report:\\n\", classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2a88b9-5646-4c61-a9c1-de9e4ead7fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_evaluation = evaluate_model(pipeline, X_train, y_train, X_test, y_test)\n",
    "pp_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab1568f-c022-4d6c-8cf3-f1bf90ad172a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion matrix for test prediction\n",
    "\n",
    "cm_pp = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "plot_confusion_matrix(\n",
    "    cm_pp,\n",
    "    target_names=['Good Loan', 'Bad Loan'], \n",
    "    title='Confusion Matrix',\n",
    "    normalize=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efdb5f5-f019-441d-b95e-bd01d1d691fe",
   "metadata": {},
   "source": [
    "# Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a30f457-aaf9-47a7-9c7a-c9979740ff1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df_train = pd.concat(\n",
    "    [dt_evaluation, rf_evaluation, xgb_evaluation, pp_evaluation],\n",
    "    keys=[\"Decision Tree\", \"Random Forest\", \"XGBoost\", \"STacking Random Forest & XGB\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c46994-e942-480e-8901-ee2b3c548f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df_train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
